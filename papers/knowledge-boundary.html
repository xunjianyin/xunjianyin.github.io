<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Knowledge Boundary for LLMs</title>
  <link rel="stylesheet" type="text/css" href="../stylesheet.css">
  <link rel="stylesheet" type="text/css" href="../shared-styles.css">
  <link rel="icon" type="image/png" href="../figures/logo.png">
  <style>
    .paper-container { max-width: 850px; margin: 0 auto; padding: 40px 20px; line-height: 1.8; }
    .paper-title { font-size: 32px; font-weight: bold; margin-bottom: 20px; line-height: 1.3; color: #1a1a1a; }
    .paper-authors { font-size: 16px; color: #555; margin-bottom: 8px; }
    .paper-venue { font-size: 15px; color: #888; margin-bottom: 25px; font-style: italic; }
    .paper-links { margin-bottom: 35px; }
    .paper-links a { display: inline-block; padding: 10px 20px; margin-right: 12px; margin-bottom: 10px; background: #2d5a3d; color: white; text-decoration: none; border-radius: 6px; font-size: 14px; font-weight: 500; transition: all 0.2s; }
    .paper-links a:hover { background: #1e3d2a; transform: translateY(-1px); }
    .paper-figure { margin: 40px 0; text-align: center; }
    .paper-figure.hero img { max-width: 100%; width: 700px; }
    .paper-figure.medium img { max-width: 550px; width: 100%; }
    .paper-figure img { border-radius: 10px; box-shadow: 0 4px 20px rgba(0,0,0,0.1); }
    .paper-figure figcaption { font-size: 14px; color: #666; margin-top: 15px; max-width: 650px; margin-left: auto; margin-right: auto; line-height: 1.6; }
    .story-section { margin-bottom: 35px; }
    .story-section h2 { font-size: 22px; color: #2d5a3d; margin-bottom: 15px; font-weight: 600; }
    .story-section p { color: #333; margin-bottom: 15px; text-align: justify; }
    .highlight-box { background: linear-gradient(135deg, #f8faf8 0%, #eef4ee 100%); padding: 25px 30px; border-radius: 10px; margin: 30px 0; border-left: 4px solid #2d5a3d; }
    .highlight-box h3 { margin-top: 0; color: #2d5a3d; font-size: 18px; }
    .highlight-box ul { margin-bottom: 0; padding-left: 20px; }
    .highlight-box li { margin-bottom: 8px; color: #444; }
    .concept-box { background: linear-gradient(135deg, #f0f4ff 0%, #e8eeff 100%); padding: 25px 30px; border-radius: 10px; margin: 30px 0; border-left: 4px solid #4a5a8d; }
    .concept-box h3 { margin-top: 0; color: #4a5a8d; font-size: 18px; }
    .back-link { margin-bottom: 30px; }
    .back-link a { color: #2d5a3d; text-decoration: none; font-weight: 500; font-size: 15px; }
    .back-link a:hover { text-decoration: underline; }
    .paper-citation { background: #f5f5f5; padding: 20px; border-radius: 8px; font-family: 'Monaco', 'Menlo', monospace; font-size: 12px; overflow-x: auto; white-space: pre-wrap; margin-top: 40px; }
    .paper-citation h3 { font-family: inherit; margin-top: 0; margin-bottom: 15px; font-size: 16px; color: #333; }

    /* Dark mode styles */
    [data-theme="dark"] .paper-title { color: var(--text-color); }
    [data-theme="dark"] .paper-authors { color: #b8b8b8; }
    [data-theme="dark"] .paper-venue { color: #999; }
    [data-theme="dark"] .story-section h2 { color: var(--primary-color); }
    [data-theme="dark"] .story-section p { color: var(--text-color); }
    [data-theme="dark"] .paper-figure figcaption { color: #b8b8b8; }
    [data-theme="dark"] .highlight-box { background: linear-gradient(135deg, #1a2a1a 0%, #1e3a1e 100%); border-left-color: var(--primary-color); }
    [data-theme="dark"] .highlight-box h3 { color: var(--primary-color); }
    [data-theme="dark"] .highlight-box li, [data-theme="dark"] .highlight-box p { color: var(--text-color); }
    [data-theme="dark"] .back-link a { color: var(--primary-color); }
    [data-theme="dark"] .paper-citation { background: var(--section-bg); color: var(--text-color); }
    [data-theme="dark"] .paper-links a { background: var(--primary-color); }
    [data-theme="dark"] .paper-links a:hover { background: #004a8c; }
  </style>
</head>
<body>
  <nav class="nav-buttons" role="navigation" aria-label="Main navigation">
    <a href="../index.html" class="nav-button">Home</a>
    <a href="../publications.html" class="nav-button">Publications</a>
    <a href="../projects.html" class="nav-button">Projects</a>
    <a href="../blogs.html" class="nav-button">Blogs</a>
    <a href="../photography.html" class="nav-button">Photography</a>
    <button id="theme-toggle" class="theme-toggle" aria-label="Toggle dark mode" title="Toggle dark/light mode"></button>
  </nav>

  <main class="paper-container">
    <div class="back-link"><a href="../index.html">← Back to Home</a></div>

    <h1 class="paper-title">Benchmarking Knowledge Boundary for Large Language Model: A Different Perspective on Model Evaluation</h1>
    <p class="paper-authors"><b>Xunjian Yin*</b>, Xu Zhang*, Jie Ruan, Xiaojun Wan</p>
    <p class="paper-venue">ACL 2024</p>

    <div class="paper-links">
      <a href="https://aclanthology.org/2024.acl-long.124/">Paper</a>
      <a href="https://github.com/pkulcwmzx/knowledge-boundary">Code</a>
    </div>

    <div class="story-section">
      <h2>The Prompt Sensitivity Problem</h2>
      <p>Ask a language model a question, and it might get it right. Rephrase the same question, and suddenly it fails. This prompt sensitivity is one of the most frustrating aspects of working with LLMs—a model's apparent knowledge depends not just on what you ask, but on how you phrase it.</p>
      <p>This creates a fundamental problem for evaluation. When we test a model's knowledge with a fixed set of questions, are we measuring what the model knows, or just how well the prompts happen to match what it learned? The distinction matters enormously.</p>
    </div>

    <div class="concept-box">
      <h3>Introducing Knowledge Boundary</h3>
      <p>We propose a new concept: the <em>knowledge boundary</em>. Rather than asking whether a model can answer a specific prompt, we ask: what is the full range of prompts for which this knowledge is accessible? The boundary encompasses both prompt-agnostic knowledge (accessible regardless of phrasing) and prompt-sensitive knowledge (accessible only with specific wordings).</p>
    </div>

    <div class="story-section">
      <h2>A Robuster Evaluation</h2>
      <p>Traditional benchmarks evaluate models on fixed question sets. If the model knows the answer but the exact phrasing happens to trigger a failure, the benchmark marks it wrong. If the model gets lucky with a phrasing, it marks it right. Neither result reflects true knowledge.</p>
      <p>Knowledge boundary evaluation searches for the optimal prompt for each piece of knowledge. This gives a more reliable measure of what the model actually knows, independent of superficial prompt variations.</p>
    </div>

    <div class="story-section">
      <h2>Finding the Boundary</h2>
      <p>How do you find the optimal prompt for a given piece of knowledge? We developed a projected gradient descent algorithm with semantic constraints. The algorithm searches the space of possible prompts, looking for phrasings that maximize the model's ability to retrieve the correct answer while staying semantically equivalent to the original question.</p>
      <p>This is harder than it sounds—the search space is enormous, and we need to ensure the discovered prompts are actually asking the same question, not gaming the model with unrelated triggers.</p>
    </div>

    <div class="highlight-box">
      <h3>Key Contributions</h3>
      <ul>
        <li><b>New Framework:</b> Knowledge boundary as a prompt-robust evaluation concept</li>
        <li><b>Algorithm:</b> Projected gradient descent with semantic constraints to find optimal prompts</li>
        <li><b>Better Evaluation:</b> More reliable assessment of model knowledge across domains</li>
        <li><b>Insights:</b> Revealing the gap between apparent and actual model knowledge</li>
      </ul>
    </div>

    <div class="story-section">
      <h2>What We Learned</h2>
      <p>Our experiments revealed that knowledge boundaries vary dramatically across models and domains. Some knowledge is robustly accessible; other knowledge hangs by a thread, retrievable only with very specific prompts. Understanding these boundaries helps us build more reliable systems and design better evaluation protocols.</p>
      <p>Knowledge boundary offers a new lens on model capabilities—one that sees past the noise of prompt sensitivity to measure genuine understanding.</p>
    </div>

    <h3 style="margin-bottom: 15px; color: #333;">Citation</h3>
    <div class="paper-citation">@inproceedings{yin-etal-2024-benchmarking,
    title = "Benchmarking Knowledge Boundary for Large Language Models: A Different Perspective on Model Evaluation",
    author = "Yin, Xunjian and Zhang, Xu and Ruan, Jie and Wan, Xiaojun",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics",
    year = "2024",
    url = "https://aclanthology.org/2024.acl-long.124/",
    pages = "2270--2286"
}</div>
  </main>

  <!-- Footer -->
  <footer class="site-footer" style="max-width:850px;margin:0 auto;padding:20px;">
    <div class="footer-social">
      <a href="https://github.com/Arvid-pku" aria-label="GitHub" target="_blank" rel="noopener">
        <svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" viewBox="0 0 24 24" fill="currentColor"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg>
      </a>
      <a href="https://www.linkedin.com/in/xunjian-yin-5b40252a5" aria-label="LinkedIn" target="_blank" rel="noopener">
        <svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" viewBox="0 0 24 24" fill="currentColor"><path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"/></svg>
      </a>
      <a href="https://x.com/xunjian_yin" aria-label="Twitter" target="_blank" rel="noopener">
        <svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" viewBox="0 0 24 24" fill="currentColor"><path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/></svg>
      </a>
      <a href="https://scholar.google.com/citations?user=PociQ5EAAAAJ" aria-label="Google Scholar" target="_blank" rel="noopener">
        <svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" viewBox="0 0 24 24" fill="currentColor"><path d="M5.242 13.769L0 9.5 12 0l12 9.5-5.242 4.269C17.548 11.249 14.978 9.5 12 9.5c-2.977 0-5.548 1.748-6.758 4.269zM12 10a7 7 0 1 0 0 14 7 7 0 0 0 0-14z"/></svg>
      </a>
    </div>
    <p class="footer-copyright">© 2025 Xunjian Yin. All rights reserved.</p>
  </footer>

  <!-- Back to Top Button -->
  <button id="back-to-top" class="back-to-top" aria-label="Back to top">
    <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><polyline points="18 15 12 9 6 15"></polyline></svg>
  </button>

  <script src="../utils.js"></script>
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      initializeDarkMode();
      initializeBackToTop();
    });
  </script>
</body>
</html>
