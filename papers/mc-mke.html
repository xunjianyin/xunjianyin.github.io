<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MC-MKE: Multimodal Knowledge Editing</title>
  <link rel="stylesheet" type="text/css" href="../stylesheet.css">
  <link rel="stylesheet" type="text/css" href="../shared-styles.css">
  <link rel="icon" type="image/png" href="../figures/logo.png">
  <style>
    .paper-container { max-width: 850px; margin: 0 auto; padding: 40px 20px; line-height: 1.8; }
    .paper-title { font-size: 32px; font-weight: bold; margin-bottom: 20px; line-height: 1.3; color: #1a1a1a; }
    .paper-authors { font-size: 16px; color: #555; margin-bottom: 8px; }
    .paper-venue { font-size: 15px; color: #888; margin-bottom: 25px; font-style: italic; }
    .paper-links { margin-bottom: 35px; }
    .paper-links a { display: inline-block; padding: 10px 20px; margin-right: 12px; margin-bottom: 10px; background: #2d5a3d; color: white; text-decoration: none; border-radius: 6px; font-size: 14px; font-weight: 500; transition: all 0.2s; }
    .paper-links a:hover { background: #1e3d2a; transform: translateY(-1px); }
    .paper-figure { margin: 40px 0; text-align: center; }
    .paper-figure.hero img { max-width: 100%; width: 700px; }
    .paper-figure.medium img { max-width: 550px; width: 100%; }
    .paper-figure img { border-radius: 10px; box-shadow: 0 4px 20px rgba(0,0,0,0.1); }
    .paper-figure figcaption { font-size: 14px; color: #666; margin-top: 15px; max-width: 650px; margin-left: auto; margin-right: auto; line-height: 1.6; }
    .story-section { margin-bottom: 35px; }
    .story-section h2 { font-size: 22px; color: #2d5a3d; margin-bottom: 15px; font-weight: 600; }
    .story-section p { color: #333; margin-bottom: 15px; text-align: justify; }
    .highlight-box { background: linear-gradient(135deg, #f8faf8 0%, #eef4ee 100%); padding: 25px 30px; border-radius: 10px; margin: 30px 0; border-left: 4px solid #2d5a3d; }
    .highlight-box h3 { margin-top: 0; color: #2d5a3d; font-size: 18px; }
    .highlight-box ul { margin-bottom: 0; padding-left: 20px; }
    .highlight-box li { margin-bottom: 8px; color: #444; }
    .back-link { margin-bottom: 30px; }
    .back-link a { color: #2d5a3d; text-decoration: none; font-weight: 500; font-size: 15px; }
    .back-link a:hover { text-decoration: underline; }
    .paper-citation { background: #f5f5f5; padding: 20px; border-radius: 8px; font-family: 'Monaco', 'Menlo', monospace; font-size: 12px; overflow-x: auto; white-space: pre-wrap; margin-top: 40px; }
    .paper-citation h3 { font-family: inherit; margin-top: 0; margin-bottom: 15px; font-size: 16px; color: #333; }
  </style>
</head>
<body>
  <nav class="nav-buttons">
    <a href="../index.html" class="nav-button">Home</a>
    <a href="../publications.html" class="nav-button">Publications</a>
    <a href="../projects.html" class="nav-button">Projects</a>
    <a href="../blogs.html" class="nav-button">Blogs</a>
  </nav>

  <main class="paper-container">
    <div class="back-link"><a href="../index.html">← Back to Home</a></div>

    <h1 class="paper-title">MC-MKE: A Fine-Grained Multimodal Knowledge Editing Benchmark Emphasizing Modality Consistency</h1>
    <p class="paper-authors">Junzhe Zhang, Huixuan Zhang, <b>Xunjian Yin</b>, Baizhou Huang, Xu Zhang, Xinyu Hu, Xiaojun Wan</p>
    <p class="paper-venue">ACL 2025 Findings</p>

    <div class="paper-links">
      <a href="https://aclanthology.org/2025.findings-acl.896/">Paper</a>
      <a href="https://github.com/reroze/MC-MKE">Code</a>
    </div>

    <div class="story-section">
      <h2>When Vision and Language Disagree</h2>
      <p>Multimodal language models can see images and understand text—but what happens when their knowledge about the two becomes inconsistent? A model might know that the Eiffel Tower is in Paris from text, but misidentify it in an image. Or it might correctly recognize a person's face while getting their name wrong.</p>
      <p>These inconsistencies matter. As we deploy multimodal models in real applications, we need to be able to correct their knowledge—to edit what they believe. But multimodal knowledge editing is harder than it sounds.</p>
    </div>

    <div class="story-section">
      <h2>Two Types of Errors</h2>
      <p>We realized that multimodal errors aren't monolithic. They fall into two distinct categories: <em>misreading</em> errors, where the model processes the visual information correctly but retrieves wrong textual knowledge, and <em>misrecognition</em> errors, where the visual processing itself fails.</p>
      <p>This distinction is crucial for knowledge editing. Fixing a misreading error requires editing textual knowledge associations. Fixing a misrecognition error requires correcting visual representations. Previous benchmarks conflated these, making it impossible to diagnose what's actually going wrong.</p>
    </div>

    <div class="highlight-box">
      <h3>The Modality Consistency Challenge</h3>
      <p>When you edit multimodal knowledge, you want the change to be consistent across modalities. If you correct a model's knowledge about the Eiffel Tower, it should get the answer right whether you ask in text or show an image. But we found that existing editing methods often fail at this—they might fix the text pathway while leaving the visual pathway broken, or vice versa.</p>
    </div>

    <div class="story-section">
      <h2>Building MC-MKE</h2>
      <p>We constructed MC-MKE as a fine-grained benchmark that explicitly separates misreading and misrecognition scenarios. Each test case is designed to probe a specific type of error and verify whether editing achieves consistency across both modalities.</p>
      <p>The benchmark allows researchers to diagnose exactly where their editing methods succeed and fail, providing the granularity needed to develop better techniques.</p>
    </div>

    <div class="highlight-box">
      <h3>Key Findings</h3>
      <ul>
        <li><b>Modality Gap:</b> Current methods struggle to maintain consistency between visual and textual knowledge after editing</li>
        <li><b>Error-Specific Needs:</b> Different error types require different editing strategies for optimal correction</li>
        <li><b>Benchmark Value:</b> MC-MKE reveals significant room for improvement in multimodal knowledge editing</li>
        <li><b>Future Directions:</b> The work motivates research into modality-aware editing techniques</li>
      </ul>
    </div>

    <div class="story-section">
      <h2>The Path Forward</h2>
      <p>As multimodal models become more prevalent, the ability to correct and update their knowledge becomes increasingly important. MC-MKE provides the foundation for developing editing techniques that respect the multimodal nature of these systems—ensuring that when we fix what a model knows, the fix is complete and consistent.</p>
    </div>

    <h3 style="margin-bottom: 15px; color: #333;">Citation</h3>
    <div class="paper-citation">@inproceedings{zhang-etal-2025-mc,
    title = "{MC}-{MKE}: A Fine-Grained Multimodal Knowledge Editing Benchmark Emphasizing Modality Consistency",
    author = "Zhang, Junzhe and Zhang, Huixuan and Yin, Xunjian and Huang, Baizhou and Zhang, Xu and Hu, Xinyu and Wan, Xiaojun",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2025",
    year = "2025",
    url = "https://aclanthology.org/2025.findings-acl.896/",
    pages = "17430--17445"
}</div>
  </main>
</body>
</html>
